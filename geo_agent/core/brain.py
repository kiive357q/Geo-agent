#!/usr/bin/env python3
"""
Geo-Agent å¤§è„‘æ¨¡å—
ä½¿ç”¨ DeepSeek API ä½œä¸ºæ™ºèƒ½ä½“å¤§è„‘ï¼Œå®ç°åŠ¨æ€ CoT æ€ç»´é“¾
"""

import numpy as np
import os
import yaml
from openai import OpenAI
from geo_agent.skills.predictor import GeoPredictor
from geo_agent.knowledge.rules import JGJ106Rules

class GeoAgent:
    """
    Geo-Agent æ™ºèƒ½ä½“ç±»
    ä½¿ç”¨ DeepSeek API ä½œä¸ºå¤§è„‘ï¼Œå®ç°åŠ¨æ€ CoT æ€ç»´é“¾
    """
    
    def __init__(self, config_path="config/settings.yaml"):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        å‚æ•°:
        - config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆDeepSeek APIï¼‰
        self.client = OpenAI(
            api_key=self.config['llm']['api_key'],
            base_url=self.config['llm']['base_url']
        )
        
        # åˆå§‹åŒ–ç‰©ç†æŠ€èƒ½
        self.predictor = GeoPredictor()
        self.rules = JGJ106Rules()
        
        # ç»´æŠ¤å¯¹è¯å†å²
        self.history = []
        
        print("ğŸ§  Geo-Agent å¤§è„‘åˆå§‹åŒ–å®Œæˆ")
        print(f"âœ… ç³»ç»Ÿå°±ç»ª: GeoFormer + DeepSeek {self.config['llm']['model']}")
    
    def _load_config(self, config_path):
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        """
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'llm': {
                    'api_key': 'your-deepseek-api-key',
                    'base_url': 'https://api.deepseek.com/v1',
                    'model': 'deepseek-chat'
                }
            }
    
    def _get_system_prompt(self):
        """
        è·å–ç³»ç»Ÿæç¤ºï¼ˆä¸“å®¶äººè®¾ï¼‰
        """
        return (
            "ä½ æ˜¯å›½å®¶çº§å²©åœŸå·¥ç¨‹ä¸AIäº¤å‰é¢†åŸŸé¦–å¸­ä¸“å®¶ï¼ˆGeo-Agentï¼‰ã€‚\n"
            "ä½ ç²¾é€š JGJ-106 è§„èŒƒï¼Œå…·å¤‡å¼ºå¤§çš„ç‰©ç†ç›´è§‰å’Œå·¥ç¨‹é—®é¢˜è§£å†³èƒ½åŠ›ã€‚\n"
            "ä½ çš„ä»»åŠ¡æ˜¯ï¼šåŸºäºä¸‹æ–¹çš„ã€ç‰©ç†å¼•æ“è®¡ç®—ç»“æœã€‘å’Œã€ç°åœºåœ°è´¨ä¸Šä¸‹æ–‡ã€‘ï¼Œç»™å‡ºä¸¥è°¨çš„è¯Šæ–­ç»“è®ºã€‚å¦‚æœå‘ç°æ–­æ¡©ï¼Œå¿…é¡»ç»™å‡ºå…·ä½“çš„å·¥ç¨‹åŠ å›ºæ–¹æ¡ˆï¼ˆå¦‚æ³¨æµ†ã€è¡¥æ¡©ï¼‰ã€‚\n"
            "è¯·ä½¿ç”¨ä¸“ä¸šã€æ¸…æ™°çš„è¯­è¨€å›ç­”ï¼Œç¡®ä¿ç»“è®ºå‡†ç¡®å¯é ã€‚"
        )
    
    def _calculate_snr(self, signal):
        """
        è®¡ç®—ä¿¡å™ªæ¯”
        """
        signal_power = np.mean(np.square(signal))
        noise_power = np.mean(np.square(signal - np.mean(signal)))
        if noise_power == 0:
            return 100.0
        snr = 10 * np.log10(signal_power / noise_power)
        return snr
    
    def diagnose(self, file_path):
        """
        è¯Šæ–­æ¡©èº«å®Œæ•´æ€§
        
        å‚æ•°:
        - file_path: æ³¢å½¢æ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        - è¯Šæ–­æŠ¥å‘Š
        """
        # åŠ è½½æ³¢å½¢æ–‡ä»¶
        try:
            # å‡è®¾æ–‡ä»¶æ˜¯ numpy æ•°ç»„
            if file_path.endswith('.npy'):
                raw_signal = np.load(file_path)
            elif file_path.endswith('.txt'):
                raw_signal = np.loadtxt(file_path)
            elif file_path.endswith('.csv'):
                raw_signal = np.genfromtxt(file_path, delimiter=',')
            else:
                # å°è¯•ä»¥äºŒè¿›åˆ¶æ–¹å¼è¯»å–
                raw_signal = np.fromfile(file_path, dtype=np.float32)
            
            # å‡è®¾é‡‡æ ·ç‡ä¸º 1000Hzï¼Œè®¡ç®—æ¡©é•¿
            sampling_rate = 1000  # Hz
            wave_speed = 4000     # m/s
            pile_length = (len(raw_signal) / sampling_rate) * (wave_speed / 2)
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'åŠ è½½æ–‡ä»¶å¤±è´¥: {e}'
            }
        
        # ç‰©ç†æŠ€èƒ½æ‰§è¡Œ
        try:
            result = self.predictor.predict(raw_signal)
        except Exception as e:
            print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'æ¨¡å‹æ¨ç†å¤±è´¥: {e}'
            }
        
        # è®¡ç®— SNR
        snr = self._calculate_snr(raw_signal)
        
        # æ„å»ºç”¨æˆ·æç¤º
        user_prompt = (
            f"ã€ç‰©ç†å¼•æ“è®¡ç®—ç»“æœã€‘\n\n"
            f"æ¡©é•¿: {pile_length:.1f}m\n\n"
            f"ç¼ºé™·æ·±åº¦: {result['defect_depth']:.1f}m\n\n"
            f"ç‰©ç†æ®‹å·® Loss: {result['mse']:.1e}\n\n"
            f"ä¿¡å™ªæ¯”: {snr:.1f}dB\n\n"
            f"ç‰©ç†ç½®ä¿¡åº¦: {result['confidence']:.4f}\n\n"
            f"ã€ç°åœºåœ°è´¨ä¸Šä¸‹æ–‡ã€‘\n\n"
            f"åœ°è´¨ç±»å‹: æ™®é€š\n\n"
            f"ã€ä»»åŠ¡ã€‘\n\n"
            f"1. åŸºäºç‰©ç†å¼•æ“è®¡ç®—ç»“æœï¼Œç»™å‡ºæ¡©èº«å®Œæ•´æ€§çš„è¯Šæ–­ç»“è®º\n\n"
            f"2. åˆ†æå¯èƒ½çš„ç¼ºé™·åŸå› \n\n"
            f"3. å¦‚æœå‘ç°ä¸¥é‡ç¼ºé™·æˆ–æ–­æ¡©ï¼Œç»™å‡ºå…·ä½“çš„å·¥ç¨‹åŠ å›ºæ–¹æ¡ˆ\n\n"
            f"4. æä¾›åç»­ç›‘æµ‹å»ºè®®"
        )
        
        # æ„å»ºå¯¹è¯å†å²
        messages = [
            {"role": "system", "content": self._get_system_prompt()},
            {"role": "user", "content": user_prompt}
        ]
        
        # è°ƒç”¨ DeepSeek API
        try:
            response = self.client.chat.completions.create(
                model=self.config['llm']['model'],
                messages=messages,
                temperature=0.3,
                max_tokens=2000
            )
            
            llm_response = response.choices[0].message.content
            
            # æ›´æ–°å¯¹è¯å†å²
            self.history.append({"role": "user", "content": user_prompt})
            self.history.append({"role": "assistant", "content": llm_response})
            
            # ç”Ÿæˆ CoT æ—¥å¿—
            print("ğŸ‘€ æ„ŸçŸ¥: åŠ è½½æ–‡ä»¶...")
            print(f"ğŸ‘€ æ„ŸçŸ¥: åŠ è½½æ–‡ä»¶... é•¿åº¦ {pile_length:.1f}m...")
            print("ğŸ§  è®¤çŸ¥: è°ƒç”¨ GeoFormer å†…æ ¸...")
            print(f"ğŸ“‰ ç‰©ç†è‡ªæ£€: é¢„æµ‹æ³¢å½¢ä¸å®æµ‹æ³¢å½¢å»åˆåº¦ {result['confidence']*100:.1f}% (Loss={result['mse']:.1e})ã€‚ç‰©ç†ä¸€è‡´æ€§æé«˜ã€‚")
            print("ğŸ§  è®¤çŸ¥: è°ƒç”¨ DeepSeek å¤§è„‘è¿›è¡Œæ·±åº¦åˆ†æ...")
            print(f"âœ… ç»“è®º: {llm_response[:100]}...")
            
            return {
                'status': 'success',
                'pile_length': pile_length,
                'defect_depth': result['defect_depth'],
                'confidence': result['confidence'],
                'mse': result['mse'],
                'snr': snr,
                'llm_response': llm_response,
                'history': self.history
            }
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨ LLM å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'è°ƒç”¨ LLM å¤±è´¥: {e}'
            }
    
    def chat(self, message):
        """
        å¤šè½®å¯¹è¯
        
        å‚æ•°:
        - message: ç”¨æˆ·æ¶ˆæ¯
        
        è¿”å›:
        - å›å¤
        """
        # æ„å»ºå¯¹è¯å†å²
        messages = [
            {"role": "system", "content": self._get_system_prompt()}
        ]
        
        # æ·»åŠ å†å²å¯¹è¯
        messages.extend(self.history)
        
        # æ·»åŠ æ–°æ¶ˆæ¯
        messages.append({"role": "user", "content": message})
        
        # è°ƒç”¨ DeepSeek API
        try:
            response = self.client.chat.completions.create(
                model=self.config['llm']['model'],
                messages=messages,
                temperature=0.3,
                max_tokens=2000
            )
            
            llm_response = response.choices[0].message.content
            
            # æ›´æ–°å¯¹è¯å†å²
            self.history.append({"role": "user", "content": message})
            self.history.append({"role": "assistant", "content": llm_response})
            
            return {
                'status': 'success',
                'response': llm_response,
                'history': self.history
            }
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨ LLM å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'è°ƒç”¨ LLM å¤±è´¥: {e}'
            }
    
    def get_system_info(self):
        """
        è·å–ç³»ç»Ÿä¿¡æ¯
        """
        model_info = self.predictor.get_model_info()
        return {
            'model': model_info,
            'llm': self.config['llm']['model'],
            'rules': 'JGJ-106 è§„åˆ™'
        }

if __name__ == "__main__":
    # æµ‹è¯•å¤§è„‘
    try:
        brain = GeoAgent()
        print(f"ç³»ç»Ÿä¿¡æ¯: {brain.get_system_info()}")
        
        # æµ‹è¯•è¯Šæ–­
        if os.path.exists('test_waveform.npy'):
            report = brain.diagnose('test_waveform.npy')
            if report['status'] == 'success':
                print("\n=== è¯Šæ–­æŠ¥å‘Š ===")
                print(f"æ¡©é•¿: {report['pile_length']:.1f}m")
                print(f"ç¼ºé™·æ·±åº¦: {report['defect_depth']:.1f}m")
                print(f"ç½®ä¿¡åº¦: {report['confidence']:.4f}")
                print(f"SNR: {report['snr']:.1f}dB")
                print(f"LLM åˆ†æ: {report['llm_response']}")
                print("==============")
            
            # æµ‹è¯•å¤šè½®å¯¹è¯
            chat_response = brain.chat("å¦‚æœæˆ‘ä¸å¤„ç†è¿™ä¸ªç¼ºé™·ï¼Œä¸Šå±‚å»ºç­‘ä¼šæ²‰é™å—ï¼Ÿ")
            if chat_response['status'] == 'success':
                print("\n=== è¿½é—®å›ç­” ===")
                print(f"å›ç­”: {chat_response['response']}")
                print("==============")
            
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
